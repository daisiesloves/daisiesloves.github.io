{
    "version": "https://jsonfeed.org/version/1",
    "title": "It's better to burn out than to fade away.",
    "subtitle": null,
    "icon": "https://daisiesloves.github.io/images/favicon.ico",
    "description": null,
    "home_page_url": "https://daisiesloves.github.io",
    "items": [
        {
            "id": "https://daisiesloves.github.io/Kafka-topic%E9%85%8D%E7%BD%AE%E9%94%99%E8%AF%AF%E6%8A%A5%E9%94%99%E4%BF%A1%E6%81%AF/",
            "url": "https://daisiesloves.github.io/Kafka-topic%E9%85%8D%E7%BD%AE%E9%94%99%E8%AF%AF%E6%8A%A5%E9%94%99%E4%BF%A1%E6%81%AF/",
            "title": "Kafka topic配置错误报错信息",
            "date_published": "2022-10-16T10:09:16.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"kafka错误日志如下原因是kafka-topic信息配置错误\"><a class=\"anchor\" href=\"#kafka错误日志如下原因是kafka-topic信息配置错误\">#</a> Kafka 错误日志如下，原因是 kafka topic 信息配置错误</h1>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">16 Jun 2022 14:32:12,697 INFO  [New I/O server boss #11] (org.apache.avro.ipc.NettyServer$NettyServerAvroHandler.handleUpstream:171)  - [id: 0x8a66e568, /10.209.9.3:49176 =&gt; /10.209.36.55:6572] OPEN</span><br><span class=\"line\">16 Jun 2022 14:32:12,698 INFO  [New I/O worker #1] (org.apache.avro.ipc.NettyServer$NettyServerAvroHandler.handleUpstream:171)  - [id: 0x8a66e568, /10.209.9.3:49176 =&gt; /10.209.36.55:6572] BOUND: /10.209.36.55:6572</span><br><span class=\"line\">16 Jun 2022 14:32:12,698 INFO  [New I/O worker #1] (org.apache.avro.ipc.NettyServer$NettyServerAvroHandler.handleUpstream:171)  - [id: 0x8a66e568, /10.209.9.3:49176 =&gt; /10.209.36.55:6572] CONNECTED: /10.209.9.3:49176</span><br><span class=\"line\">16 Jun 2022 14:33:14,494 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.handleCompletedMetadataResponse:968)  - [Producer clientId=producer-1] Error while fetching metadata with correlation id 9 : &#123;jyucmall_info_sc_3=UNKNOWN_TOPIC_OR_PARTITION&#125;</span><br><span class=\"line\">16 Jun 2022 14:33:14,494 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: o-pvNtyoQ-eqxIKUVdjRRA</span><br><span class=\"line\">16 Jun 2022 14:33:15,115 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.handleCompletedMetadataResponse:968)  - [Producer clientId=producer-1] Error while fetching metadata with correlation id 15 : &#123;jyucmall_info_sc_3=UNKNOWN_TOPIC_OR_PARTITION&#125;</span><br><span class=\"line\">16 Jun 2022 14:33:15,905 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.handleCompletedMetadataResponse:968)  - [Producer clientId=producer-1] Error while fetching metadata with correlation id 23 : &#123;jyucmall_info_sc_3=UNKNOWN_TOPIC_OR_PARTITION&#125;</span><br></pre></td></tr></table></figure></p>\n",
            "tags": [
                "kafka"
            ]
        },
        {
            "id": "https://daisiesloves.github.io/Hadoop%20fs%E5%91%BD%E4%BB%A4/",
            "url": "https://daisiesloves.github.io/Hadoop%20fs%E5%91%BD%E4%BB%A4/",
            "title": "Hadoop fs命令",
            "date_published": "2022-10-09T03:30:06.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h1 id=\"hdfs-fs命令\"><a class=\"anchor\" href=\"#hdfs-fs命令\">#</a> HDFS fs 命令</h1>\n<ol>\n<li>\n<p>Hdfs 文件相关操作</p>\n<h2 id=\"查看目录下文件\"><a class=\"anchor\" href=\"#查看目录下文件\">#</a> 查看目录下文件</h2>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hdfs dfs -ls -h /user/jyupay</span><br><span class=\"line\">[upays@jzhjf-core-hadoop3-c-1 ~]$ hadoop fs -ls /user/jyupay/offline/diffAndsettle/docment/backup/20221006/dc_dm_charge_diff_0077_day/CHARGE20221005100001.CMCC.0077-all</span><br><span class=\"line\">-rw-r-----   3 jyupay bdocadm  159239323 2022-10-07 01:01 /user/jyupay/offline/diffAndsettle/docment/backup/20221006/dc_dm_charge_diff_0077_day/CHARGE20221005100001.CMCC.0077-all #第二列数字3为副本数</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"查看文件占用空间大小\"><a class=\"anchor\" href=\"#查看文件占用空间大小\">#</a> 查看文件占用空间大小</h2>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hdfs dfs -du -h /user/jyupay/</span><br><span class=\"line\">或 hadoop fs -du -h /user/jyupay</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"创建-删除文件夹\"><a class=\"anchor\" href=\"#创建-删除文件夹\">#</a> 创建、删除文件夹</h2>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hadoop fs -mkdir /pathname</span><br><span class=\"line\">hadoop fs -rm -r /pathname</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"创建-删除-移动文件\"><a class=\"anchor\" href=\"#创建-删除-移动文件\">#</a> 创建、删除、移动文件</h2>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hadoop fs -touchz /user/jyupay/file1.txt\t#创建一个空文件</span><br><span class=\"line\">hadoop fs -rm /user/jyupay/file1.txt\t\t#删除文件</span><br><span class=\"line\">hadoop fs -mv hdfs://ns1/user/jyupay/.Trash/file1.txt hdfs://ns1/user/jyupay/file1/file1.txt #移动文件，如从回收站恢复文件</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"永久删除文件不放入回收站\"><a class=\"anchor\" href=\"#永久删除文件不放入回收站\">#</a> 永久删除文件，不放入回收站</h2>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hdfs dfs -rm -skipTrash hadoop fs -rm -r /dir_name</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"上传-下载文件\"><a class=\"anchor\" href=\"#上传-下载文件\">#</a> 上传、下载文件</h2>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hadoop fs -put file1 /user/jyupay</span><br><span class=\"line\">hadoop fs -copyFromLocal file1.txt /user/jyupay #copyFromLocal，从本地拷贝一个文件到hdfs</span><br><span class=\"line\">hadoop fs -get /user/jyupay/file1</span><br><span class=\"line\">hadoop fs -copyToLocal /user/jyupay/file1 /opt/data/file1 #copyToLocal，从hdsf拷贝文件到本地</span><br><span class=\"line\">hadoop fs -getmerge /user/jyupay/1.txt /user/jyupay/2.txt 3.txt #getmerge合并hdfs多个文件并传至本地</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"查看文件\"><a class=\"anchor\" href=\"#查看文件\">#</a> 查看文件</h2>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hadoop fs -cat /user/jyupay/file1.txt</span><br><span class=\"line\">hadoop fs -cat /user/jyupay/file*</span><br><span class=\"line\">hadoop fs -tail /user/jyupay/file2.txt\t#tail查看文件的最后1000字节</span><br><span class=\"line\">hadoop fs -text /user/jyupay/file3.txt\t#将源文件输出为文本格式。允许的格式是zip和TextRecordInputStream，如果是压缩文件会先解压再查看</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"hdfs清空回收站\"><a class=\"anchor\" href=\"#hdfs清空回收站\">#</a> Hdfs 清空回收站</h2>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hadoop fs -ls -r hdfs://ns1/user/jyupay/.Trash</span><br><span class=\"line\">hadoop fs -expunge</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>查看目录配额相关<br />\n hadoop fs -count [-q] [-h] [-v] [-t [&lt;storage type&gt;]] [-u] [-x] [-e] &lt;path&gt;<br />\n 功能描述：<br />\n统计 HDFS 指定路径的可容纳文件 / 文件夹配额、空间配额、目录数、文件数及占用空间<br />\n选项解释：<br />\n-q：输出统计列：<br />\nQUOTA（指定路径可建文件 / 文件夹数量配额）<br />\nREM_QUOTA（指定路径可建文件 / 文件夹数量剩余配额）<br />\nSPACE_QUOTA（指定路径可建文件 / 文件夹空间配额）<br />\nREM_SPACE_QUOTA（指定路径可建文件 / 文件夹空间剩余配额）<br />\nDIR_COUNT（指定路径下文件夹（包括自身）统计数）<br />\nFILE_COUNT（指定路径下文件统计数）<br />\nCONTENT_SIZE（指定路径下文件及文件夹大小总和）<br />\nPATHNAME（指定路径）<br />\n-u：输出统计列：<br />\nQUOTA（指定路径可建文件 / 文件夹数量配额）<br />\nREM_QUOTA（指定路径可建文件 / 文件夹数量剩余配额）<br />\nSPACE_QUOTA（指定路径可建文件 / 文件夹空间配额）<br />\nREM_SPACE_QUOTA（指定路径可建文件 / 文件夹空间剩余配额）<br />\nPATHNAME（指定路径）<br />\n-h：把数据单位显示为容易理解的单位，比如空间原单位为 byte，加入该参数后显示 k，m，g 等单位<br />\n - v：显示标题行<br />\n - t：显示每种存储类型的空间配额和使用情况。如果未给出 - u 或 - q 选项，则 - t 选项将被忽略。可选参数 storage type 支持的输入类型有：&quot;all&quot;，&quot;ram_disk&quot;，&quot;ssd&quot;，&quot;disk&quot; 或 &quot;archive&quot;。<br />\n-x：统计结果排除快照。如果指定了 - u 或 - q 选项，则 - x 选项将被忽略<br />\n - e：显示指定路径的 EC 编码模式，通过列 ERASURECODING_POLICY 显示</p>\n<h2 id=\"查看hdfs空间配额信息\"><a class=\"anchor\" href=\"#查看hdfs空间配额信息\">#</a> 查看 hdfs 空间配额信息</h2>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hadoop fs -count -q -v /user/bdoc/35/hive/jyupay</span><br><span class=\"line\">或 hdfs dfs -count -q -v /user/jyupay/</span><br></pre></td></tr></table></figure></p>\n</li>\n</ol>\n",
            "tags": [
                "hadoop",
                "hdfs"
            ]
        },
        {
            "id": "https://daisiesloves.github.io/Hdfs%E7%9B%AE%E5%BD%95%E9%85%8D%E9%A2%9D%E7%A9%BA%E9%97%B4%E4%B8%8D%E5%A4%9F%E5%AF%BC%E8%87%B4Spark%E4%BB%BB%E5%8A%A1%E5%BC%82%E5%B8%B8/",
            "url": "https://daisiesloves.github.io/Hdfs%E7%9B%AE%E5%BD%95%E9%85%8D%E9%A2%9D%E7%A9%BA%E9%97%B4%E4%B8%8D%E5%A4%9F%E5%AF%BC%E8%87%B4Spark%E4%BB%BB%E5%8A%A1%E5%BC%82%E5%B8%B8/",
            "title": "HDFS目录配额空间不够导致Spark任务异常",
            "date_published": "2022-10-08T07:30:06.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h2 id=\"hdfs目录配额空间不够导致spark任务异常\"><a class=\"anchor\" href=\"#hdfs目录配额空间不够导致spark任务异常\">#</a> HDFS 目录配额空间不够导致 Spark 任务异常</h2>\n<h3 id=\"错误日志如下\"><a class=\"anchor\" href=\"#错误日志如下\">#</a> 错误日志如下：</h3>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">22/10/07 03:45:18 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.</span><br><span class=\"line\">22/10/07 03:45:21 INFO Client: Uploading resource file:/tmp/spark-a23912f6-850d-45a5-88d2-a2a4376c03d9/__spark_libs__5308838579049091591.zip -&gt; hdfs://router-fed/user/jyupay/.sparkStaging/application_1663262516437_821787/__spark_libs__5308838579049091591.zip</span><br><span class=\"line\">22/10/07 03:45:22 INFO Client: Uploading resource file:/app01/upays/data-center/pay/combine/data_combine.jar -&gt; hdfs://router-fed/user/jyupay/.sparkStaging/application_1663262516437_821787/data_combine.jar</span><br><span class=\"line\">22/10/07 03:45:22 INFO Client: Deleted staging directory hdfs://router-fed/user/jyupay/.sparkStaging/application_1663262516437_821787</span><br><span class=\"line\">Exception in thread &quot;main&quot; org.apache.hadoop.hdfs.protocol.DSQuotaExceededException: The DiskSpace quota of /user/jyupay is exceeded: quota = 27487790694400 B = 25 TB but diskspace consumed = 27488154349032 B = 25.00 TB</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyStoragespaceQuota(DirectoryWithQuotaFeature.java:195)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:222)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:1159)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:991)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:950)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addBlock(FSDirWriteFileOp.java:505)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.saveAllocatedBlock(FSDirWriteFileOp.java:777)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.storeAllocatedBlock(FSDirWriteFileOp.java:260)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2728)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:894)</span><br><span class=\"line\">        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:581)</span><br><span class=\"line\">        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:529)</span><br><span class=\"line\">        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1036)</span><br><span class=\"line\">        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:917)</span><br><span class=\"line\">        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:854)</span><br><span class=\"line\">        at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class=\"line\">        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1699)</span><br><span class=\"line\">        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2779)</span><br></pre></td></tr></table></figure></p>\n",
            "tags": [
                "hdfs",
                "spark"
            ]
        },
        {
            "id": "https://daisiesloves.github.io/Spark%E5%90%AF%E5%8A%A8%E6%97%B6%E6%97%A5%E5%BF%97Kafka%E6%B6%88%E8%B4%B9%E6%8A%A5%E9%94%99/",
            "url": "https://daisiesloves.github.io/Spark%E5%90%AF%E5%8A%A8%E6%97%B6%E6%97%A5%E5%BF%97Kafka%E6%B6%88%E8%B4%B9%E6%8A%A5%E9%94%99/",
            "title": "Spark启动时日志Kafka消费报错",
            "date_published": "2022-09-16T07:30:06.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"spark启动报错日志\"><a class=\"anchor\" href=\"#spark启动报错日志\">#</a> Spark 启动报错日志</h3>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">22/09/16 06:00:29 INFO CachedKafkaConsumer: Initial fetch for spark-executor-own_combine2022082406 jyupay_py_1 7 28846987218</span><br><span class=\"line\">22/09/16 06:00:29 INFO KerberosLogin: [Principal=jyupay@ZHKDC]: TGT valid starting at: Fri Sep 16 06:00:29 CST 2022</span><br><span class=\"line\">22/09/16 06:00:29 INFO KerberosLogin: [Principal=jyupay@ZHKDC]: TGT expires: Sat Sep 17 06:00:29 CST 2022</span><br><span class=\"line\">22/09/16 06:00:29 INFO KerberosLogin: [Principal=jyupay@ZHKDC]: TGT refresh sleeping until: Sat Sep 17 02:19:28 CST 2022</span><br><span class=\"line\">22/09/16 06:00:29 INFO AppInfoParser: Kafka version : 2.0.0</span><br><span class=\"line\">22/09/16 06:00:29 INFO AppInfoParser: Kafka commitId : 3402a8361b734732</span><br><span class=\"line\">22/09/16 06:00:29 INFO CachedKafkaConsumer: Initial fetch for spark-executor-own_combine2022082406 jyupay_py_1 0 29119882113</span><br><span class=\"line\">22/09/16 06:00:29 INFO Metadata: Cluster ID: o-pvNtyoQ-eqxIKUVdjRRA</span><br><span class=\"line\">22/09/16 06:00:29 INFO Metadata: Cluster ID: o-pvNtyoQ-eqxIKUVdjRRA</span><br><span class=\"line\">22/09/16 06:00:29 ERROR Executor: Exception in task 4.1 in stage 0.0 (TID 14)</span><br><span class=\"line\">org.apache.kafka.clients.consumer.OffsetOutOfRangeException: Offsets out of range with no configured reset policy for partitions: &#123;jyupay_py_1-7=28846987218&#125;</span><br><span class=\"line\">        at org.apache.kafka.clients.consumer.internals.Fetcher.parseCompletedFetch(Fetcher.java:970)</span><br><span class=\"line\">        at org.apache.kafka.clients.consumer.internals.Fetcher.fetchedRecords(Fetcher.java:490)</span><br><span class=\"line\">        at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1259)</span><br><span class=\"line\">        at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1187)</span><br><span class=\"line\">        at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1115)</span><br><span class=\"line\">        at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.poll(CachedKafkaConsumer.scala:99)</span><br><span class=\"line\">        at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.get(CachedKafkaConsumer.scala:70)</span><br><span class=\"line\">        at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:223)</span><br><span class=\"line\">        at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:189)</span><br><span class=\"line\">        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)</span><br><span class=\"line\">        at com.cmsz.join.PayChargeMain$.formatMessageCharge(PayChargeMain.scala:126)</span><br><span class=\"line\">        at com.cmsz.join.PayChargeMain$$anonfun$3.apply(PayChargeMain.scala:66)</span><br><span class=\"line\">        at com.cmsz.join.PayChargeMain$$anonfun$3.apply(PayChargeMain.scala:66)</span><br><span class=\"line\">        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)</span><br><span class=\"line\">        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)</span><br><span class=\"line\">        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)</span><br><span class=\"line\">        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)</span><br><span class=\"line\">        at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)</span><br><span class=\"line\">        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)</span><br><span class=\"line\">        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)</span><br><span class=\"line\">        at org.apache.spark.scheduler.Task.run(Task.scala:121)</span><br><span class=\"line\">        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)</span><br><span class=\"line\">        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1367)</span><br><span class=\"line\">        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)</span><br><span class=\"line\">        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)</span><br><span class=\"line\">        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)</span><br><span class=\"line\">        at java.lang.Thread.run(Thread.java:745)</span><br></pre></td></tr></table></figure></p>\n",
            "tags": [
                "kafka"
            ]
        },
        {
            "id": "https://daisiesloves.github.io/Flume-1.10%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%99/",
            "url": "https://daisiesloves.github.io/Flume-1.10%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%99/",
            "title": "Flume-1.10启动异常",
            "date_published": "2022-08-26T01:30:32.000Z",
            "content_html": "<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"flume-110unsupported-majorminor-version-520启动异常\"><a class=\"anchor\" href=\"#flume-110unsupported-majorminor-version-520启动异常\">#</a> Flume-1.10,Unsupported major.minor version 52.0 启动异常</h3>\n<pre><code>执行报错：Exception in thread &quot;main&quot; java.lang.UnsupportedClassVersionError: com/heima/socket/Demo2_Receive : Unsupported major.minor version 52.0\n\n]$ java -version\njava version &quot;1.7.0_91&quot;\nOpenJDK Runtime Environment (rhel-2.6.2.3.el7-x86_64 u91-b00)\nOpenJDK 64-Bit Server VM (build 24.91-b01, mixed mode)\n\n经分排查，是生产环境jdk版本过低导致，将jdk版本升级到1.8，即可解决此报错问题。使用jd-gui工具，查看META-INF\\MANIFEST.MF中的内容，Build-Jdk属性就是jar包，JDK的版本。\n\n在flume-env.sh配置文件，指定jdk路径\nexport JAVA_HOME=&quot;/home/upays/jdk1.8.0_211/</code></pre>\n",
            "tags": [
                "flume",
                "jdk"
            ]
        }
    ]
}